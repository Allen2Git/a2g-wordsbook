Hi. Good morning. I'm Mark Broker and welcome Thio. The first p o a Talk of the year. This is my
ninth p o. A talk. And for the last couple of years, I've been doing the first talk of the year to
kind of kicked. The Siri's often get things going on. There's a great year to come of interesting
speakers and fantastic technical and leadership content. So very, very excited about that. Um, let's
get going today. I'm gonna be talking about virtualization. Virtualization. Is this technology
that's pretty core to everything. We do it AWS but has been used in Amazon for a very long time.
Virtualization is away for one computer to pretend to be many computers on, uh, might be thinking,
Well, why would you do this, right? Like we have lots of computers. Why don't we just use each one
for one thing and there are a whole bunch of reasons that we want to do this We call his multi
tendency having multiple workloads on a single box. One of them is just practical, modern service,
that really big. They have tens, of course, often hundreds of gigabytes of ram and many of our
workers just aren't that big. The smallest lambda function is 128 megabytes. I gotta buy a server
with 128 megabytes of RAM. I can buy a DIV board with 128 megabytes of RAM, and so I want to take a
box and cut it up into into the right size slices for the workload. The other reason we like multi
tendency is perhaps a little bit less obvious. And that is it has really nice statistical properties
that help us reduce the costs of running the cloud and offer our customers lower prices and lower
costs. And I know for a business lower costs. And the reason for that is if we take the workloads
that spike up in the first minute of the hour and the world close that spike up in the second minute
to the hour and the ones who spike up in the fourth minute to the hour on, we peck those onto one
box. We end up with a workload that is Maur busy over Maur time, so we keep the box busier and
keeping the box busier means that we need less hardware, and this hardware means that we're spending
this money, and that is value that we can pass on to our customers. It also means that we can offer
more consistent laden see to our customers because the workload, because our machines are less
likely to get saturated right, they're less likely to run against that 100% CPU, 100% network
utilization or or whatever, and so we can give people a higher quality off service. So when we think
about cutting boxes up, there are a whole bunch of things we care about security things and
operational things, and so on. Perhaps, but perhaps the most important one is this idea of
isolation, and we say it must be safe for multiple things. And those things could be Lambda
functions. They could be containers. They could be easy to instances to run on the same hardware
protected against privilege, escalation, information disclosure and cover channel or cover channels
and other risks. Let's step into each of those privilege. Escalation means that when we give a
workload some rights, we say this workload can access this better the desk or this better memory or
the set of network addresses or whatever else. We don't want that workload, no matter how evil it
is, no matter what it does to be able to increase the amount of rights that it has to be able to
take things that don't belong to it. So there's obviously very, very important in a multi tenant
environment. The other obvious one is information disclosure, where we have multiple tenants on a
machine. It is extremely important that the tenants can't look at take or in further data of other
tenants. And that's because we have different customers, workloads on a machine or even different
work loads of different sensitivities to our own businesses. And we don't want to have to trust
every piece of code running on the machine. So instead, we want to build these boundaries that are
strong boundaries against the flow of information and the slightly less obvious minus covert
channels. We don't want it to be possible for one tenant and another tenant, even if they cooperate
to transfer data from one to the other. And why don't why do we care about that? What we care about
that because in AWS and in order of our infrastructure We offer customers a wide variety of
authorization controls, right? There's I am the security groups, the network Ackles. They're all of
these ways for customers to say, Let this stuff talk to this stuff. But don't let this thing talk to
the Internet, for example. But if if we're close contest ablest covert channels, they can move data
out across thes authorization boundaries without being suitably authorized, and that is obviously a
very bad thing. So we want even intentional information disclosure, which these covert channels to
be impossible. So you might be thinking, Well, we solved this problem in the 19 sixties with
processes don't processes do that too, right? I can run 10 things on my laptop. I can run 1000
things on a server, Um, and yes, to an extent, processes do solve this problem and have attempted to
solve this problem for a very long time. But it turns out that there are some very difficult
tradeoffs in trying to secure to build a security boundary it around processes. So, talking about to
get into why those tradeoffs Let's talk a little bit about the way that code interacts with the
Colonel. So here we have it. The bottom. The host Colonel. For most of us, that's a UNIX kernel. But
it might be Windows or BSD or whatever. And on top of there is running some piece of interested code
and that code interacts with the Colonel. It interacts with the Colonel through Siskel's. Which of
these kind of a P I calls you make it to the colonel. Read this file, right, That file open this
file. You know, seeing this network packet very useful staff and interact with Colonel through
special files. You know, they're things like Prock insists these directories for the files on the
file system that are kind of magical. You can read things from them, you can write to them and the
colonel does cool stuff on your behalf. It's a very powerful programming model. They also interacted
the Colonel in less are explicit ways. When you lead from a file, the colonel will read a chunk of
data from their file, and it will cash it. When you're right, it'll put your right into a buffer and
then we'll go into a dirty page cash. So they all these data structures, buffers and caches and
things inside any operating system colonel that our code interacts with, and this interaction is
incredibly complex. So if you install a typical the next destroy, the study was done on a version of
a bunting from a couple of years ago. It interacts with the colonel in a vast number of ways 224
system calls just for the standard installation. So that's 224 unique AP eyes that the Colonel has
to expose to run this workbook. And that's on top off 208 of these things called I Octa LS and so
on. Their special verbs that go into one system called hate make it behave like multiple system
calls. Because apparently Colonel developers don't like designing AP eyes. Eso, really there are
sort of 400 something system calls that this workload does. There are hundreds of special files, and
that's on top of the page caches and buffers, and someone and Martin clinics exposes something
between 354 100 Siskel's toe work clothes, so that is a huge surface area. It's a huge surface area
from a operational perspective, and it's a huge surface area from a security perspective, and that's
very difficult to reason about, huh? As people got into the kind of container world they started
thinking about. Well, what can we do to limit the surface area? To draw a line that limits the
amount of stuff that interested code could do against the colonel? And that is the fundamental idea
off sand boxing, bowling, a layer of control around a process that limits what it could do with the
outside world and typically in Lennox is implemented with a thing called sitcom PPF, which is a kind
of firewall for the colonel. And, you can say, does process control? Call these, says calls. But not
those schools. Very useful stuff. And this is the foundation of security off. Typical thing, typical
installations of things like Dhaka. But there's a really ugly trade off here and a tradeoff that is
particularly kind of against the things we like to do at Amazon. And that is, there is a hard trade
off between security and feature richness. There's a hard trade off where you have to say to that
code. There are the 356 calls most of them are useful. They aren't there just for, you know, for
lost. They're there because the colonel developers put them in for good reasons. But you can't use
most of them. You know, you can see in your GP packets you can open a role socket, whatever the case
may be. And that's bad in a bunch of ways, because it limits what customers can build on multi 10
and systems that work this way. And it forces out into a really hard set of tradeoffs between
compatibility with what customers want to bring, whether that's legacy code or new development and
security. And we never want to be in that situation. We never want to be making that trade off
because really hard to win. It's been a couple of solutions to that over the years. Uh huh, a couple
of solutions to that over the years. One of them is this idea of a library OS or an emulator, and
this is the model of Google users with G Visor Onda. Huge number of research and production systems
have used, and this is the idea that you take your interested code instead of running that directly
on the colonel. You intercept those those calls and things in some way. You re implement them in
user space, and that re implementation uses a much smaller surface area of the colonel, and this is
a cool model. It's actually very useful. There are a lot of things to like about this model, but one
of the risks of this model is that is a re implementation. It's a re implementation off millions of
lines of operating system code. And so as you develop their code, you have to keep up with all of
the features that are being added top rating systems. But you also need to keep up with all the bags
because there's anyone who's ever offered in a P. I knows your users on building. There's programs
against the documentation of your A P I. They're building their programs against the behavior of
your A P I, and there's a huge amount of behavior there. So this is really hard. It's really hard to
get this right to the point that you can run all programs really reliably. It's possible, but
difficult. So can we avoid this? Can we avoid these two hard problems by kind of going one layer
down going into the basement underneath the operating system into the hardware layer and pretending
instead of pretending to be a layer about the OS Colonel pretending to be a layer below the OAS
colonel between those Colonel and on the hardware. And that's exactly what we do with
virtualization. So as I go through this talk, I'm gonna present one architecture for virtualization.
There are not of ways of solving this problem. There are a lot of different approaches to skinning
this particular cat, but this is one that I've chosen because this is the one that Lambda uses in
production on is very, very popular. So here's the high level model. We take that interested code.
We ran it on top of an unmodified guest, colonel. No sand boxing needed between them. And then we
draw a box. Now we don't trust that colonel anymore because it's just interacting with all that
interested code. So we draw a box around that colonel, and we run that in a way that we intercept
all of the all of the work that it does on hardware and provide a virtual virtual hardware service
is to them and this is implemented in with two pieces. One of those pieces is called K V M. This is
a next feature, said Call the Colonel Virtual Machine. She'll talk about little bit detail, and then
there is the V M M. And this is a user space program. It runs outside the box on the guest and
provides a whole bunch of service is into that guests. Let's dive into how that works. But first,
let's talk about why this is effective if you take the example of Firecracker, which is the V mm
that we use to run Lambda functions that there firecracker V mm needs less than 46 schools. So we've
reduced their colonel surface area by something like 80%. And most importantly, those 40 Siskel's
are the popular obvious ones. Open read right Cind Message received message. And so one there, none
of the weird ones. And it turns out that I hope there's a whole bunch of good operating system
research that shows that the bag density in popular AP eyes and popular says calls is way lower than
the bag density and unpopular parts of the Colonel. And you know that you have an intuition for this
in your own systems, the code parts that get used every day are matchless, likely to be baggy than
the ones that get cold very infrequently. And so this is very successful. It's a successful model of
isolation, so computers excuse me. Computers typically do two things. One of those things is compute
stuff, right, and this is the vast majority of what they do. Computing stuff is adding numbers,
multiplying numbers, shifting stuff, getting things from memory, riding things to memory and so on
and computers to a vast amount of this. The code that you've written gets compiled down into
essentially basic arithmetic on different hamburgers, and the CPU streams through that basic
arithmetic at great speed. And that speed is very important because computers are doing so. Match of
this. You want to run their computer code at native speed as fast as it would run on a normal CPU.
And that means you can't have a software layer, right, like a condiment emulator between the clothes
that you're running and the actual hardware. And so that problem gets solved their bunch of ways to
solve it. But one of the common ways to solve it is with with features in the hardware and in this
case, specifically there's a feature in Intel. Process is called V T X, and that's a set of virtual
ization extensions. AMG processes do this to modern arm processes. Do this to pretty much every
major process. Architecture has this kind of features. It by now rejects is great, but it's a very,
very low level set of state of primitives. So I'm not really gonna talk about it matching talk.
Instead, we're gonna talk through a through K V M and K V EMS Job is to set up this HVT X for us is
to provide an abstraction layer around the VT. X that helps us sit things out. And once we've set
things up, we can start running that guest computer code, and their computer code will run at full
speed nearly full speed because there's no interposing layer, it's really running on the silicon, we
said to the CPU, you know, build this box for us and run this stuff inside the spots at full speed
directly on the silicon, and they know software layers there. So computers fan. But actually not
particularly useful. The thing that makes computers useful is I R and input an output. Getting
things from users giving feedback to users. Sending Network Peck. It's receiving network Peck. It's
showing things on the screen. It's set of computers. Couldn't do that. We wouldn't even be bothering
with them. And so we have this guest. It's in a box. It's got a limited amount of memory. It's
running on the CPU, but their boxes doesn't provide any other. You know any other things to it? So
how do we do? I owe we do I owe with the help of this user space process the V mm Okay. Diem's other
job beyond sitting up B T X is to provide an A p I for writing these v m. M's writing this users
base code that lets guests do I R. So to get an idea of how this works and to really understand it,
let's build a V M m. It's a package in Brazil if you wanna play along at home. I called in Brooklyn
N O. V mm, which is a very basic and well commented implementation of this stuff. Certainly not
production quality, though, Um, so probably the least interesting thing of view. Member is sitting
things up and this is a This is a fairly simple process. We create of'em. We say to Katya, make a
box for me. We allocate some memory by by calling in map so that his memory from the host Oh, let's
remember there's a process in the host So we allocate memory when we give it to the V. M. We say
this used to be my my memory. There's now the PM's memory. It can run on that. Then we create some
V. C. P uses the virtual CPU that will actually kind of run the code. And we can create any number
of these. Michael creates one because I don't want to mess around with threads. We set up CPU
registers, um, on DDE that is essentially setting things up in the CPU to say, you know, your memory
is here. The code you need to load is there. And so once, just a little bit of housekeeping and then
we start running. We call this call this a P I K v m run that says, make that vm go often run and
normally when you build one of these things. They think that you're running would be an OS Colonel,
right? You would want to boot Lennox or boot being your boot windows or whatever, but those kernels
are extremely complicated. So let's simplify this down to the simplest possible guest. Here's my
someplace possible guest. This guest is written in X 86 Assembly and if you aren't familiar with
that, I kind of stepped through it a little bit on our first line. We're taking the special variable
called A. Is this a general purpose? CPU Register off, which there are a handful with special names
and we're sitting it to the value 2020. We're just taking 2020 and we're moving it into that
register. Then we've got a jump label. Luke, this is just like a go to label in C or Java. Whatever.
The next line, we dec criminal x X equals X minus one. But something very cool is happening under
the covers here because the CPU has a has a whole bunch of other things that it sets other than a X
when it does this arithmetic operation and those things were called flags and there are flags about
the result of the arithmetic operation. And one of those flags is zero flag. It was kind of this
hidden variable inside the CPU that win the last arithmetic operation Return zero It sits the zero
flag, and then we use that on the next line with this instruction JMC jump, if not zero. And all
that does is says if the zero flag is empty. So the lost arithmetic operation didn't return. Zero
jump to Luke. And so we've got this loop that will spin around 2020 times. And on the last passed
through decay, Excell will sit extra zero that that zero flag will get set and we'll drop through
this jump operation. We'll just do nothing. And then we get to a whole corporation holds kind of a
special thing. I not gonna touch our. But what it does do is pause control back to the V. Mm. So
let's look at this from the V m m's perspective, we call that TV um ran a p I run our code. Then we
handed control over to the guest and the guest rend this entire loop without the V mm. Knowing about
it right there's no software going on here. It'll the stuff is running directly on the silicon in
the CPU. Until we got to that Holt instruction and that Holt instruction says to the sea for you,
give control back to the V. Mm. It's one of many ways to give control back to the V. Mm. And then
the vehement gets that cave. Ian ran returns. This is called a V M exit, and I talk about X. It's a
little bit during this talk and in all cases, que viene knows our implementation knows we're done.
There are other ways to handle that, and we're done. So that's hard runs between the guests and the
V. Mm. So I talked about little bit earlier about how the V mm does I R. On That's its main job in
the world that does this little bit of set up, but it does. I own handles the Io path in this model
of virtual ization. But before we get there, we need to talk a little bit about virtual memory. So
way back in the 19 fifties and 19 sixties, computers essentially could run one program at a time,
right, and that was fine because imagination for what we would do with computers was fairly limited.
But after a while, people started saying, Actually, I want to run multiple programs and switch
between them quickly, or I want to run programs that are bigger than the memory on this machine,
especially in memory, was hilariously expensive, billions of times more expensive than it is now.
And so I want to run this program that is bigger than physical memory. How do I do that? And there
was a huge debate at the time and a lot of controversy. And most most people said, Well, the way you
do that is you make your programmers program really carefully to be ableto handle the case that they
memory might actually be in memory or might be on desk and on every memory access you have to check
and so on. And programmers are lazy, so we should just make them work harder. You might have heard
that another context, too, and, it turns out, likely those people lost the hardware people one and
the hardware people built. This idea, called virtual memory into the hardware and virtual memory, is
a hardware accelerated in direction layer of indirection between the addresses that your program
seas and the actual physical addresses. And this is cool. This is cool because it lets us around
multiple things on the same machine without them being able to run into each other because the CPU
can also trapped when things try and scribble outside the lines of the virtual memory they have
allocated to them, Admits has take the physical memory and move it around, move it around in Ram,
move alone to disc, move it out over the network, compress it. Whatever you want to do, encrypted
whatever you want to do with it without the program having to know about that because we have this
in direction layer that could do that for us. And that is super cool. Extremely flexible stuff now,
obviously, in the virtual ization world way have two layers that want to use virtual memory. We have
our host colonel, which needs to run all the stuff that runs, including that fi mm on our guest
colonel, which needs to run all the stuff it brands, including all of its guests. And so modern
virtualization staff supports nested virtual memories to kind of virtual memory. Entire virtual
memory, which looks like this right? We have in the gray are some process on the guest with its
pages. It uses regular virtual memory. The colonel's none the wiser to the guest, Colonel, the guest
colonel's physical memory. Not really. Physical memory is backed by virtual memory belonging to the
V. Mm. Remember we called in that, but we got a chunk of virtual memory in the V mm, which again is
backed by virtual allocations off the host memory. So they're two layers of indirection here, and
that gives us even more flexibility to do cool stuff on because of the hardware accelerated. It's
pretty fast. So I talked about how if you're right outside the kind of scribble outside the lines of
your virtual memory are your VM will trap or your CPU will tap that and give the Colonel the offer.
The Theo opportunity to do something with that. Generally, if you just do that in a program, you
write the colonel. What the Colonel will do with its opportunity is send you a signal and then kill
you. You can't behave like that. It's bad, but there's another cool thing that you could do with
virtual memory, and that is memory math. Taya. You can take a device right that could be like a
sound card or a mouse or a network card or a hard drive or whatever and make it pretend to be a
chunk of memory. And then when the colonel sees those reads and writes from that special chunk of
memory, affords them to the device. And this actually happens at at the physical level and the
virtual level, and it's a very popular way to do I owe into interact with devices. There are lots of
other ways, as is usual with these things. But there's a popular in pretty good way because
interacting with memory is something it feels natural, and it's easy, and this mechanism allows off.
The M M, which remember controls, has full control of the the memory layout of the guest to pretend
to be a hardware device because it can get those reads and writes, because every time you read and
write outside that area of map memory, the host colonel will chap it and then say the V mm, what do
you want to do with us? And if you're doing it in just the right way. The V mm could trap those and
pretend to respond to the or respond to those in a way that pretends to be a physical piece of
hardware. This is very cool. So let's look at that. What? This looks like it the code level. We've
set up our memory layout so that 1000 just arbitrary number I chose is the address off a virtual
device. So the first thing we do is we move that address 1000 into one of our general purpose
registers. And then we read from that those brackets around B X or like a pointer, appointed you
reference saying treat be exes appointer rather than as a a cz an actual number. So move one bite
out of the location of B X into the low bite off eggs. So read one bite from the virtual device. And
as soon as that happens, the host colonel sees that traps it and gives it to the V M M. K v. Imran
returns and says your ex Idris in his cave iam exit. Mm I r. And that gives the V M M the option to
decide what to do, and it looks like the physical address and since 1000 and that's a magic address
I know, and one of the things that's allowed to do is respond to that memory. Read with data, and
here we respond to that memory. Read with data by calling Get C by reading a character from the
terminal. And so what we've done is we've built a virtual hardware device, which reads characters
from the host terminal and feeds them one by one to the guest vehement Com'on restart. And the guest
could do something with their character. Um, in the States that's build is the basic pattern behind
a pretty much arbitrary amount of device magic because their fee mm, it's a full process running
inside our host. It could do pretty much anything at once. You could handle it locally. It could go
over the network to a network service. It could go to a distributed system and say, Handle this for
me. It could go thio another core in the machine or another process on the machine. And this lets
has build very cool stuff. Is that a virtual hardware inside AWS? So if you look at something like E
b s, right? It's a virtual hard drive, and for the most part, to the guest. It looks like a piece of
hardware, but is actually not a piece of hardware sitting on local hardware. It's this complex
distributed system that offers great performance and durability and all of this other staff. And so
this is really cool. Abstraction layer that It's complex distributed systems pretend to be hardware.
And for provided like aws that's magically powerful. So what's the first thing that we do when we
get new hardware? We make a game. Making games is fun, but I'm pretty bad at it. So we're gonna make
the simplest game we can. I'm gonna use that get see device to read something from from the Colonel,
and that's gonna cause a VM exit. Then I'm gonna create a new device. In my view, ma'am, which they
random letter device, the returns, a random byte. Then my guest program is gonna compare and those
two things and calculate the score. And then I'm gonna add a new device in my V. Mm. That does put C
one character at a time right to the terminal, right and then I'm gonna go to one. So I've made a
super great game. Says gets a letter I enter. See? It's a good letter to guess I get. See the
random. It was H zero outta one. Correct. Um, so this game has two problems. It works. It's not a
problem, but it is both very difficult and very boring. Um, and we could get good at it right now.
What do you do in games to haul you practice, You know, we're gonna cheat. By this time, you should
be thinking why there's so many ways to cheat the V. M. M has full control over memory. It could
just change the program. ICE has full control over registers. It owns that random number generator
device, right? It has so many ways to cheat, but we're gonna cheat in a special way. That is
particularly cool. So let's go back to the fact that the virtual machine monitor the V mmm has full
control over the memory. It can read and write it. And it's full control of the CPU registers that
can read and write them so we can read the memory and read the CPU registers and put them together
in a box. We could make a copy of them, and once we have a copy of them, we can restore them. And
these two combination of things being able to save memory and save registers and restore memory and
restore registers lets us cause guests to time travel to go back in time to an earlier point in
their executions. Like guest ground Hog Day. So how are we gonna use that to cheat? It was the
structure of our game is our game loop. What we're gonna do is, uh, uh, before we go into a game
loop, we're gonna make a snapshot, gonna take a copy of the registers and memory. Then we got to go
through the game as normal. Then if we lost, which the Vehement will know because it's going to see
it print out the score, we're just going to restore that snapshot. We're gonna time travel back to
an earlier time when we didn't lose the game and the guest is none the wiser. And then so from this
perspective, we can cheat it this game and get in a way that this guest, uh, just cannot detect
because it's just time travels backwards, and every time it sees the world actually go forwards,
we've won the game. This doesn't make the game minimal fund or any easier, But it does mean that we
can rack up high scores. You have a 438 points. Fantastic. Um, so this is fun. But what we actually
do with it Well, it turns out visibility to snapshot viens and to save those snapshots is extremely
useful. There's a whole bunch of cool stuff we can do with that, right? We could make that snapshot
on. We can transfer to a different machine. This is migration. Take a workload, put it on a
different machine workload doesn't know about that. Doesn't need to know about that. So you can
build things guests and more reliable than the hardware of a physical machine. Cool. Our version of
this will pause, but it turns out that this accounting magic inside K v m it's a stewardess live
without the guest, even seeing a pause more than a couple of 1,000,000 seconds. And so, um, so K v m
s o. The guest will just see itself moved from or won't see itself. It will move from one machine to
another with essentially no ports. It's called live migration. Very powerful way to build high
availability systems in the cloud. We could also duplicate things like there's a Lego wizard, good
duplicated. We can do a bunch of expensive work, start up a J VM load all of your code, start of
spring, whatever, then snapshot. And then we can take those snapshots on. We can stash them
somewhere else. And when we need a pre built thing, we can just put that into memory, put that into
the CPU and run with it as many times as we like in parallel or in cereal. So this is very cool,
because there's a whole new mechanism to M or ties loading time in in machines, super powerful lower
level constructs. And so, throughout the history of virtual ization, this stuff has been mostly used
for taking whole operating systems like easy two instances and running them multi tenanted on
hardware, doing things like live migration. That is very cool, very economically important stuff.
But I think in the next decade virtualization is gonna get really big by beginning really small and
uh, that is going to do that by reducing the overhead bye instead of us brooding. Whole operating
systems were going to run just little bits of code inside these votes because they're really
compelling security and operational reasons that we want to use this tool kit. And there's no reason
that we should only cut up a box into eight pieces or whatever with it. We can cut up a machine into
thousands of pieces like Lambda does in production today with Firecracker, and we can cut up a
machine into tens of thousands of pieces. We can send these things around the network and so on. So
it's a very, very powerful tool kit. And I think we're just gonna be seeing Maura, Maura, Maura off
over the next decade with help from operating system builders from cloud vendors from hardware and
so one. So let's get a little bit into the real world with a couple of ah, a couple of real world
points about running this town. I mentioned firecracker a couple of times, you might be familiar
with it, but if you're not in 2018 we announced firecracker, which is a vey mm. It's this box that
runs in use of space and handles I owe it sits up. K V M. It's much better than my implementation.
We announced Firecracker, we open sourced firecracker. It's up there and get a job. It's written in
rust. I'd encourage you to go and check it out. And that is the Viet Minh that we use in production
for Lambda today, and we're starting to use it in. A whole lot of other places in AWS were also very
excited that the community has started to pick up firecracker integrated into container systems,
integrated into other service offerings and and use it in all kinds of new and creative ways. The
firecracker team, who both firecracker are doing a great job of taking some of the low level pieces
off firecracker and spitting them out into a project called Rest V Mm, which is a set of low level
libraries for building your own custom. V. M. M's we customized firecracker for surveillance and
containers. But there are a lot of other cool things you can build. Custom V M M's for and rest
of'em were building with with help from people like Intel. We're very excited about that work and
the work going on a place like IBM in any sea labs on Microsoft in on similar, similar, similar
directions. Another point that I haven't mentioned, which is very, very important to virtual ization
is the idea of side channels. We talked about these covert channels. We talked about information
leakage and virtual ization. Does a fantastic job off closing. The main channels of providing a very
strong box around were close, but it turns out that there are a lot off other ways that information
might flow. For example, there is an attack called flash and Reload. When you flash a whole bunch of
stuff out of the cash, you wait a little better than you reload it and your time. How long it took
to reload on by timing how long it took to reload, you can figure out whether somebody else's Lo has
already loaded that information to cash, and you're going for a little bit about what they're doing.
And in things like cryptography. Even in faring, a couple of bits of information is a massive
advantage. There are other attacks, like prime and Probe flash and flash on been the one. That's
that's the logo off our Spector. A couple of years ago, Specter and Melton came out and really
changed. The way that we were thinking about this stuff really changed made us worry about a lot
more side channels that come through in the micro architecture of the CPU in things like bronze
prediction. And so we've spent a huge amount of effort in AWS mitigating. These things were very
proud of the work that we've done and if you would like to hear about that and hear about these
attacks in general, Eric Brand wine did a great talking reinvent 2019 where he talked this title
speculation and leakage, timing side channels and multi tenant computing. I'd encourage you to check
it out if you're at all interested in this stuff. Another topic about V. M. M's that's very
important is that those exits are pretty slow. Every time we're doing, I oh, we ready slow things
down, and it's obviously a lot of ways to avoid that, right, Like I could not do my I o a bite at a
time. I could get a chunk of the time I could set up a whole memory structure in guest memory with
everything I wanted to do and then just use that exit to kind of bring the V M. M's doorbell and
say, Hey, here's some work for you to do And it turns out that that's how production vehement like
Firecracker fundamentally work with shared data structures between the between the guests and the
host. But easy to where performances obviously critical, so so critical to our density and the
performance that we can offer customers are does it in a different way on Dave's been two decade on
building this cool thing called the Nitro System. And so it was a great talk that Antony the Glory
didn't reinvent 2018 Uncle powering Next. Gen Easy two instances where he goes into the designer
that my nitro system, which is very cool because it's hardware accelerated this custom. Silicon is
custom hardware are very cool stuff going on there. It's a couple of takeaways. Um, one of my big
takeaway is gonna meet to take away out of all of this is when I talk to people about their careers
at Amazon or careers in the industry on guy say, what would you like to work on. You know, 90% of
people I talk to say I want to work on big distributed systems, and that's cool. I love big
distributed systems. I've been doing that for a long time. Booked on easy to Annie B s in Landen and
FBI, Gateway and all kinds of other cool big things. So I like that answer. But there's also some
really cool stuff going on in the low level, which I think is underappreciated. Just is interesting.
Just is a great, great career path. So I'd encourage you to think about that. If you think about
what should I do next, think about going into lower level stuff is a huge demand for that, and I
think this is gonna get bigger and bigger. One of the other takeaways is that this stuff is fun to
play with. I really it is. It's It's super cool, Cool tool kit, you know, check it out. But much
like cryptography, it's fun to play with, but extremely subtle and the risks of putting in
production without fully understanding what you're doing. Our very, very high decide channels, for
example, it isn't just a set of things that you can fix and go away. It's a lifestyle of following
research, following patches, talking security, researchers on fixing things in production. And so I
would encourage you to play with the stuff and learn about it if you're at all interested in it. But
it also encourage you not to put it in production yourself on this, you're willing to hire a great
deal of expertise to help you do that very securely. I think that the final takeaway is is you know,
that these low level mechanisms just go together so well with the high level distributed system
mechanisms. In a lot of ways, this is kind of a story of AWS. We've taken these low level mechanisms
like virtualization, and it's our io ve on the hardware and all this stuff, and we've hidden big
distributed systems behind those, and that allows us to provide hardware to customers. That isn't
hardware at all. It's way faster. It's way more durable. It's way more reliable and hard way can
ever be, and it's a fantastic combination, So pretty much every talk I do I recommend a book and
today I'm gonna recommend the design and implementation off the FreeBSD operating system and you're
thinking, Yeah, my team doesn't run FreeBSD. I don't know anyone who runs FreeBSD at Emma's. And why
would I bother with this book? Ah, none of that's true, but this book has some really great, very
clear, easy time knitting and descriptions off the way that operating system colonel's work, the
data structures and algorithms that Colonel do you know, used to sit out that that virtual memory,
to do things like memory allocation to do things like paging to disk, she dealing and so on. And I
just I'm not aware that is actually a equivalent that is as clear and as easy to follow in the Lenox
world that some good books there, too. But I particularly like this one for learning about operating
system colonels and how they work. You will get an evaluation in your email after you are done with
this. Talk peacefully in your evaluations. They help us make these talks better. Choose what we want
to cover over the course of the year on dhe, serve you the audience better. So please remember to
fill that stuff out. Thank you very much. Any questions might have take questions in written form
questions? No. Okay. Oh, yeah, yeah. Um, let's go with not yet as the answer to that, but we're
doing some building very cool technology in exactly that direction. Anything else? Thanks anymore.
Same time. Yes, they're just a regular operating system process running on the guest. That's one of
the cool things about this architecture of virtual ization is you can go into that host Lennox your
PS and see all those V M. M's even on top in si Ahmed CPU they're using on. You can use the normal
operational tools that you used to to manage your host environment, and it's very, very cool. It
means that there's no kind of low level thing. You have to go poking around its system
administration as you know it, and it's monitoring as you know it. And that is very cool and very
useful. What you for what? Okay, so okay. VM stands for Colonel Virtual Machine. On it is a module
built into the Lenox Colonel that provides an abstraction layer around hardware virtualization
features and provides an AP I ah, Colonel Level a p I for writing V. M m's, the V. M M is the
virtual machine monitor. In this case, it is a process that runs in the host operating system.
Interacts with K V M. It sits K v em up. And when Kevin can't do something like I owe it gives
control over to the V M m and says V mm. Please do this for me. And that's what we were doing that I
oh, well calling get sea World calling puts. You were generating random numbers. That's all stuff
the Vietnam couldn't do because I'll code Octavian Contra because it's a sort of set of low level
things. How are you? Stop guess, J. B um, you don't necessarily need to know to stop the guest. You
couldn't just stop it at any time. That little thing there, we have to stop the CPU. So you have a
consistent state of registers and memory and so on. But generally the way that these things actually
work in production, if you have some cooperation between the guests and the host with a guest as a
bunch of work and insects to the host, Yup. I'm done with that initialization. You can snap shot me
now and then says, Can you elaborate? Your summary comment via mm's are a lot more reliable than the
underlying hardware can ever be. Yeah, so it's not that the V mmm itself is reliable. It's just a
process on a single machine. It is that the combination of virtual ization and the V mmm. Lets us
exposed systems that are more reliable. So, for example, E. B s office, better durability and a
physical disc. How could we do that in a physical machine? Um, well, we kind of Kant. But with
virtualization, we can hide this you distributed system which back something like EBS or or a lot of
the other things that easy to boats on exposes itself as a single disk. And that's very cool. And
then, with stuff like live migration, we could make the end to end system, including the CPU. I look
more reliable than a single machine. So it's not that the V m M has magic properties. It's the fact
that we turn a single computer into a distributed system which has magic properties. Okay, Do you
see a future where will produce applications that are fundamentally small? Os is meant to run on
these micro virtual machines. A single purpose OS app, thinking of an OS that compiles itself as the
part of the application for or vice versa. Back to the fifties. Back to the fifties, Yeah, back to
the fifties. Like yes, kind of. Maybe. I don't know. I think there are a lot of compelling reasons
to do that, especially with things like the Wave and people bring wanting to bring Java script and
all of these things where there's, like, really compelling reasons to build strong boxes around
staff. But his vision in desktop they're also real challenges. They're like everything has to share
a screen. You want your USB stuff to work. You want the window with the right focus to get the right
input, you wanna be able to copy and paste and so on. And the same things that provide the security
make that kind of integration difficult and weird. And so is a cool research project called Cubes
OS. Either does essentially this very cool stuff. I encourage you to check it out if you're
interested, but it is still awkward and difficult. Are there any other questions in the room? Okay,
over here Yep. Can you repeat that for us? Um, yeah. How did containers compared to virtualization?
So I mean, container means many things to many people. But if you mean like Docker, it's this model,
right? It's a sandbox around the colonel that limits the state of things that that code running
inside the container can do on the box. It can only call someone, says calls. It can only see
somebody of the file system and so on. You know what's better about that? It's extremely lightweight
set of mechanisms. There's very little overhead firecracker ads about five megabytes. That's kind of
see groups set up abs, essentially nothing. The performance is pretty great. You get a lot of good
stuff at the box. What's bad about it? It's very hard to secure property. There are a lot of side
channel problems on. There's a very difficult trade off between code compatibility on security of
the system. And so that's why you know we don't like the container mechanisms for virtual for multi
tendency in AWS. Having said that, if by containing you mean deployment mechanism by container, you
mean the ah sort of ecosystem of stuff for on custom management and scheduling and so one. There is
absolutely no reason that stuff needs to use this model rather than virtualization. Some people have
this weird religion that votes on containers. I don't understand it. There is just like no good
reason to worry about that. So you can build the whole container ecosystem on top of virtual
ization, and it would be, for the most part, just better. OK, another question from online. Does
firecrackers solve the security restrictions of accessing CPU counters? I'd like to use perfect tool
to get access to CPU counters to improve performance of programs on easy to firecracker does not
solve their problem. To some extent, the nitro system solves that problem. But, you know, we talked
about side channels of perf counters. I fantastic side channel A really convenient when on and so
easy to is for very good reason being careful about that. If on some of the newest instance types,
there are some of the performance count is exposed on the middle instance times, there are a lot of
performance counters exposed. It's a good question. It's important stuff. Such a useful tool kit on
easy to is is adding more and more of that over time as they can make that really secure on really
reliable perfect question over here. A lot of us are running right in writing untrusted code on
running through firecrackers. Do you have any recommendations? Ways we can actually use that,
actually, through of May. Can you restate that, Mark? Yeah. Do I have any, you know, sort of low
level advice for You know, how this architecture makes stuff better on Nanda? Um you know, not
really. Because a large part of our job on land is trying to hide that stuff from from guests and
trying to abstract it away. I will say that today. And we're, you know, we're working hard to fix.
This probably won't be true in a year. But today there's some caveats. Like Lambda Io performance is
pretty bad. Um, I Layton's She's pretty great, but I owe throughput is not that good. So, you know,
writing and reading files will probably slower on lander than it is on easy to you know, that's
something that we know how to fix them. We're gonna fix it. And you know, Lambda does not sink to
desk. If you're going to look in the firecrackers source code, you will see that just ignores the
sink to disk flag. That's okay for us because, you know, functions are ephemeral anyway. But as
firecracker gets adopted in more places, it's something that we need to fix. So what kind of no, no
answer to a question for online. Do we have any mechanisms in our current via memes that handle
contention one vm affecting the performance of another bm running on the same underlying host? Yes.
So that's obviously you know, we ran thousands of workloads on a Lambda box, hundreds to thousands
on. We only have 48 or or sometimes little bit more physical cause in that machine. So there's
obviously the possibility of contention the way that we manage that in lamb and in lots of ways to
to cut this. But the way that we manage it is we actually make a sand a sandwich. The customers code
runs inside a insider likely container uses this colonel thing called C groups that limits how much
sleep you and time and memory. And so when it gets and then we run that inside the vert and then we
ran the vert. The Firecracker are inside another C group, which limits how much CPU and memory and
so on. It can perform and provides an extra layer of security. And that gives us three places. We
can adjust operational controls to make sure we don't overheat out boxes in context of Lambda. We
also do that, and most most importantly, we do it at the higher level. Our placement engine
constantly looks across the fleet, finds under heated machines, finds overheated machines, finds,
correlated and correlated workloads and packs things to, uh, try and drive down P 100 CPU
utilization. But Dr at average CPU utilization. So we're not saturating any of the resource is on
the box Onda again. This is kind of magic we can do at scale and is one of the few problems. This
kind of placement is one of the few problems that is actually easier at scale than it is in small
systems on dhe. That's kind of magical. We don't run into problems like that very often just because
the statistics work out better when you have more things to pack over more places. Okay, here's a
question. I think I skipped over it, But in my head, I thought I asked it. So we'll see. How do you
know to stop the guest to clone to J. M s J B M s? You did all that on? My answer was you don't have
to know. You could just do it whenever you want. With a little bit of a caveat if you have to pose
all of the cause temporarily. Um, but in real systems, that is done cooperatively between the guest
and the host. Okay, I see. I thought I had asked him, but then somebody else asked me to ask it
again. I'm like, Okay, so that kind of brings us to the end. Are there any other questions here? We
have time for probably one more. Otherwise, I want to thank you all for coming today. Thank you,
Mark. Let's give him another hand. Yeah, thank you. Next week, just for those of you that are
interested, Brian Cash Mark will be talking about migrating from CDO to AWS and some of the
challenges that we are facing and how we're getting from here to there. So hope to see you guys next
week. Tuesday we're up in Ah, the summit. Thursday's air here Wednesday morning When you can't sleep
five o'clock in the morning, we're globally online. Thanks, You guys for joining have good night.
Thank you.
